{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6773a689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.8/site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1273 sha256=5cf0e18fdcd8052243b5268cd13d48e2cdeb68ebe67262e1b871a985920a73ff\n",
      "  Stored in directory: /Users/taiki/Library/Caches/pip/wheels/75/78/21/68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b396a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c2ef342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_from_archives_page(url, df):\n",
    "    url = url\n",
    "    response = request.urlopen(url)\n",
    "    bs = BeautifulSoup(response)\n",
    "    a_all = bs.find_all(\"a\", class_=\"title\")\n",
    "    for a in a_all:\n",
    "        a_href = a.attrs[\"href\"]\n",
    "        df = get_research_paper(a_href, df)\n",
    "    \n",
    "    response.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e431400d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-42-f53923c75daa>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-42-f53923c75daa>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "def get_abst_kw(url):\n",
    "    response = request.urlopen(url)\n",
    "    soup = BeautifulSoup(response)\n",
    "    if soup is None:\n",
    "        ab = \"N/A\"\n",
    "        kw = \"N/A\"\n",
    "    else:\n",
    "        ab = soup.find(\"p\").text\n",
    "        kw = soup.find(\"section\", class_=\"keywords\")\n",
    "        if kw is None:\n",
    "            kw = \"N/A\"\n",
    "        else:\n",
    "            kw = kw.find(\"span\", class_=\"value\").text.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            print(kw)\n",
    "            break\n",
    "    ab_kw_dict = {\"abstract\": ab, \"keywords\": kw}\n",
    "    return ab_kw_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0143c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_research_paper(url, df):\n",
    "    url = url\n",
    "    response = request.urlopen(url)\n",
    "    soup = BeautifulSoup(response)\n",
    "    volume = soup.find(\"h1\").text.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "    obj_articles = soup.find_all(\"div\", class_=\"obj_article_summary\")\n",
    "    for obj in obj_articles:\n",
    "        title = obj.find(\"h3\", class_=\"title\").find(\"a\").text.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        authors = obj.find(\"div\", class_=\"authors\").text.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        link = obj.find(\"a\", class_=\"obj_galley_link pdf\")\n",
    "        if link is None:\n",
    "            link = \"N/A\"\n",
    "        else:\n",
    "            link = link.attrs[\"href\"]\n",
    "        article_page = obj.find(\"a\").attrs[\"href\"]\n",
    "        ab_kw_dict = get_abst_kw(article_page)\n",
    "        info_dict = {\n",
    "            \"volume\": volume,\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"link\": link\n",
    "        }\n",
    "        info_dict.update(ab_kw_dict)\n",
    "        \n",
    "        df = df.append(info_dict, ignore_index=True, sort=False)\n",
    "    \n",
    "    response.close()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d0c3302c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_paper = pd.DataFrame(columns = [\"volume\", \"title\", \"authors\", \"link\"])\n",
    "url = 'https://bop.unibe.ch/JEMR/issue/archive'\n",
    "while True:\n",
    "    response = request.urlopen(url)\n",
    "    bs = BeautifulSoup(response)\n",
    "    df_paper = get_info_from_archives_page(url, df_paper)\n",
    "    url = bs.find(\"a\", class_=\"next\")\n",
    "    if  url is None:\n",
    "        break\n",
    "    else:\n",
    "        url = bs.find(\"a\", class_=\"next\").attrs[\"href\"]\n",
    "response.close()\n",
    "df_paper.to_excel(\"JEMR_title_list.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4657e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
